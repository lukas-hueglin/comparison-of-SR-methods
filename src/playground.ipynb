{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground - Perform super-resolution with different methods\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1kOSUF1jnPmSTR27yNlODdlUa6nARYGmy)\n",
    "\n",
    "With this *Jupyter Notebook*, it's possible to perform super-resolution on your own images with various methods. All these methods are based on multilayer perceptrons and trained with images from the [Unsplash Dataset](https://unsplash.com/data). The Code for building the models, training and validating them is located in the [super_resolution](./super_resolution/) directory. These modules and the tensorflow libraries are imported in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from super_resolution.utils import DatasetLoader, SampleLoader, TColors\n",
    "from super_resolution.pipeline import Performer\n",
    "\n",
    "from super_resolution import presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TColors.FAIL + '\\nTensorflow' + TColors.NOTE + ' Initialisation:')\n",
    "\n",
    "GPUs = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(GPUs[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a `SampleLoader`, you can load all your images you want to upsample. Just specify the directory with your images. The `BATCH_SIZE` indicates how many images are processed in parallel. This value can be adjusted for the needs of your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "sample_loader = SampleLoader(\n",
    "    path='D:\\\\Local UNSPLASH Samples',\n",
    "    resolution=128,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, you have to specify the method you want to use. If you want, you can also train and test your own models, but here is a list of some pretrained models:\n",
    "\n",
    ">| Method name      | PNSR (Set5) | SSIM (Set5) | Description |\n",
    ">|------------------|-------|--------|----------|\n",
    ">| SRResNet         | 27.07 | 0.7781 | Original SRResNet Method from Ledig et. al with a MSE loss function.|\n",
    ">| SRResNet_Fourier | 27.40 | 0.7917 | SRResNet Method, but with a fourier loss function instead of MSE.|\n",
    ">| SRGAN            | 24.00 | 0.6534 | Original SRRGAN Method from Ledig et. al based on a generative adversarial network.|\n",
    ">| SRGAN_Limited    | 26.05 | 0.7316 | SRGAN Method, but the discriminator was stopped being trained in order to prevent vanishing gradient.|\n",
    ">| SRGAN_Fourier    | 26.90 | 0.7480 | SRGAN Method, but in addition to the VGG19 loss function a fourier loss function was added.|\n",
    "\n",
    "These can be downloaded [here](https://drive.google.com/drive/folders/1_mq2UVoqSUDUyIqaqiv5UB1SFQKFxSZF?usp=share_link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "pipeline = Performer(\n",
    "    sample_loader=sample_loader,\n",
    "    # load a pretrained framework\n",
    "    load_path='SRGAN'\n",
    ")\n",
    "\n",
    "# check the variables of pipeline\n",
    "pipeline.check()\n",
    "\n",
    "# perform superresolution\n",
    "pipeline.perform_SR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The super-resolution images should now be located in your model's <b>performer_output</b> directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdc7da4ea7599589304f3420c070762e953ef18342640ff73f85c3d3255397db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
